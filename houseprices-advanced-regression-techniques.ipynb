{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport sys\nimport subprocess\n\n# Let me first install the suitable packages!! \nsubprocess.check_call([sys.executable, '-m', 'pip', 'install',\n'numpy'])\nsubprocess.check_call([sys.executable, '-m', 'pip', 'install',\n'pandas'])\nsubprocess.check_call([sys.executable, '-m', 'pip', 'install',\n'sklearn'])\nsubprocess.check_call([sys.executable, '-m', 'pip', 'install',\n'sklearn'])\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import scale\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n'''\nHere, I will combine 3 different regression models, which are:\n\n1. Linear Regression\n2. Gradient Boost Regression\n3. Random Forest Regression\n\n\nI will use a correlation analysis between the features and the repetitive variables in each \ncolumn as the criteria for the feature selection!!\n'''\n\n# Read and transform the training data!\ndata_path=\"../input/house-prices-advanced-regression-techniques/train.csv\"\n# Note that these variables are for the feature selection and you can change them accordingly, in case you have a better r2 score.\nrepetetive_number_percentage=0.7\nhighest_correlation_coeefficiency=0.9\n\ndef data_transform(data_path):\n    # Read the data\n    data_=pd.read_csv(data_path,sep=\",\")\n    # Remove the Id number\n    data_.drop([\"Id\"], inplace=True,axis=1)\n    # Transform the objects to numeric numbers\n    label_encoder = LabelEncoder()\n    for each in range(len(data_.columns)):\n        if data_[data_.columns[each]].dtype.name==\"object\":\n            data_[data_.columns[each]] =pd.Series(label_encoder.fit_transform(data_[data_.columns[each]].to_list()))\n    return data_\ndata_=data_transform(data_path)\n\n'''\nI will do the removing of some features according to 2 things! \n1. The percentage of highest number of repetition in the variables\n2. The correlation between the variables are more than 90 percent or not?\n'''\n# Let me remove the repetitive variables!\n\ndef remover_repetetive(data_,repetetive_number_percentage):\n    index_to_remove=[]\n    for each in range(len(data_.columns)):\n        t_=data_[data_.columns[each]].mode()[0]\n        num= (data_[data_.columns[each]].value_counts()[t_])/len(data_)\n        if num>repetetive_number_percentage:\n            index_to_remove.append(each)\n    data_ = data_.drop(data_.columns[index_to_remove],axis = 1)\n    return data_,index_to_remove\ndata_,index_to_remove=remover_repetetive(data_,repetetive_number_percentage)\n\n# Now let me look at the correlations and then remove some of the similar variables!\ndef remover_correlation_eff(data_,highest_correlation_coeefficiency):\n    corr = data_.corr()\n    numbers=[]\n    for each in range(len(corr)):\n        if each!=len(corr)-1:\n            names_=corr[corr.columns[each]][each+1:]\n            for score in names_.to_list():\n                if score>highest_correlation_coeefficiency:\n                   ind_=corr[corr.columns[each]].to_list().index(score)\n                   numbers.append(ind_)\n    rem=list(set(numbers))\n    data_ = data_.drop(data_.columns[rem],axis = 1)\n    return data_,rem\ndata_,rem=remover_repetetive(data_,highest_correlation_coeefficiency)\n\n# Now, let me use these features and try to fit our data into different regression models!\n\n# Let me define the attributes and labels, split data  to training and test sets and replace missing values with the mean value!\n\ndef generate_test_train(data_):\n    Attributes_=data_[data_.columns[:-1]]\n    Labels_=data_[data_.columns[-1:]]\n    # Let me convert Attributes and Labels dataframes to numpy array and scale my data!\n    Attributes= np.array(Attributes_)\n    Attributes=scale(Attributes)\n    Labels_=np.array(Labels_)\n    # Let  me now split my data to training set and test set!\n    X_train, X_test, y_train, y_test = train_test_split(Attributes,Labels_, test_size=0.3,random_state=109) # 70% training and 30% test data!\n    # Let me replace the missing values with  the mean value using sklearn.impute function!\n    imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n    imp = imp.fit(X_train)\n    X_train= imp.transform(X_train)\n    X_test= imp.transform(X_test)\n    return X_train, X_test, y_train, y_test\nX_train, X_test, y_train, y_test= generate_test_train(data_)\n\n# Now, let me fit our data into linear regression model!\ndef linear_regression_model(X_train, y_train,y_test):\n    reg = LinearRegression().fit(X_train, y_train)\n    y_pred_lr = reg.predict(X_test)\n\n    # Let me look at the accuracy, precision and recall of my model!\n    rsquare_lr=metrics.r2_score(y_test, y_pred_lr)\n    return rsquare_lr,reg\nrsquare_lr,reg=linear_regression_model(X_train, y_train,y_test)\nprint(\"My Linear Regression R square score is:\",rsquare_lr)\n\n\n# Now, let me fit our data into random forest regression model!\ndef random_forest_regression_model(X_train, y_train,y_test):\n    regr_forest = RandomForestRegressor(max_depth=9, random_state=0)\n    regr_forest = regr_forest.fit(X_train, y_train)\n    y_pred_lr = regr_forest.predict(X_test)\n    # Let me look at the accuracy, precision and recall of my model!\n    rsquare_rf=metrics.r2_score(y_test, y_pred_lr)\n    return rsquare_rf,regr_forest\nrsquare_rf,regr_forest=random_forest_regression_model(X_train, y_train,y_test)\nprint(\"My Random Forest Regression R square score is:\",rsquare_rf)\n\n# Now, let me fit our data into\ndef gradient_boosting_regression_model(X_train, y_train,y_test):\n    regr_gradient = GradientBoostingRegressor(max_depth=4, random_state=0)\n    regr_gradient = regr_gradient.fit(X_train, y_train)\n    y_pred_gb = regr_gradient.predict(X_test)\n    # Let me look at the accuracy, precision and recall of my model!\n    rsquare_gb=metrics.r2_score(y_test, y_pred_gb)\n    return rsquare_gb,regr_gradient\nrsquare_gb,regr_gradient=gradient_boosting_regression_model(X_train, y_train,y_test)\nprint(\"My Gradient Boosting Regression R square score is:\",rsquare_gb)\n\n# For this, similar to our training data, we need to filter our testing data!\n\n# Let me transform the test data first!\n\ndata_test_path_=\"../input/house-prices-advanced-regression-techniques/test.csv\"\ndef data_transform(data_test_path_):\n    # Read the data\n    all_test = pd.read_csv(data_test_path_, sep=\",\")\n    # Remove the Id number\n    all_data= all_test.drop([\"Id\"],axis=1)\n    # Transform the objects to numeric numbers\n    label_encoder = LabelEncoder()\n    for each in range(len(all_data.columns)):\n        if all_data[all_data.columns[each]].dtype.name==\"object\":\n            all_data[all_data.columns[each]] =pd.Series(label_encoder.fit_transform(all_data[all_data.columns[each]].to_list()))\n    # Let me also take the index we used for the training data!\n    all_data = all_data.drop(all_data.columns[index_to_remove], axis=1)\n    all_data = all_data.drop(all_data.columns[rem], axis=1)\n\n    # Let me convert the test data attributes to array!\n    Attributes_test = np.array(all_data)\n    # Let me scale the attributes!\n    Attributes_test = scale(Attributes_test)\n\n    # Let me now replace the nan values with the mean!\n    imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n    imp = imp.fit(Attributes_test)\n    Attributes_test = imp.transform(Attributes_test)\n\n    return Attributes_test,all_test\nAttributes_test,all_test=data_transform(data_test_path_)\n\n# Now, we need to use this data for the testing in order to predict our labels, utilizing 3 of the algorithms!\n\ndef test_function(Attributes_test,all_test,reg,regr_forest,regr_gradient):\n    # Let me use the trained linear regression model for the prediction!\n    y_tested_linear = reg.predict(Attributes_test)\n    all_test[\"Predictions_linear\"]=pd.DataFrame(y_tested_linear)\n\n    # Let me use the trained random forest regression model for the prediction!\n    y_tested_forest = regr_forest.predict(Attributes_test)\n    all_test[\"Predictions_forest\"]=pd.Series(y_tested_forest)\n\n    # Let me use the trained gradient boost regression model for the prediction!\n    y_tested_gradient = regr_gradient.predict(Attributes_test)\n    all_test[\"Predictions_gradient\"]=pd.Series(y_tested_gradient)\n\n    # Let me get the median of our predictions for the 3 of the algorithms!!\n    all_test[\"SalePrice\"]=all_test[[\"Predictions_linear\",\"Predictions_forest\",\"Predictions_gradient\"]].median(axis=1)\n    result_frame=all_test[[\"Id\",\"SalePrice\"]]\n\n    # Let me convert the result data frame to csv file\n    result_frame.to_csv(\"../working/sample_submission.csv\",sep=\",\",index_label=False,index=False)\n\n    return all_test\nall_test=test_function(Attributes_test,all_test,reg,regr_forest,regr_gradient)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-10T20:56:13.404206Z","iopub.execute_input":"2022-07-10T20:56:13.405334Z","iopub.status.idle":"2022-07-10T20:56:17.496315Z","shell.execute_reply.started":"2022-07-10T20:56:13.405206Z","shell.execute_reply":"2022-07-10T20:56:17.495364Z"},"trusted":true},"execution_count":1,"outputs":[]}]}